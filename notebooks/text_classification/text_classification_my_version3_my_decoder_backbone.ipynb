{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import tiktoken\n",
    "\n",
    "from model.vanila_decoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data parameters\n",
    "dataset_name=\"fancyzhx/ag_news\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\"\n",
    "model_name_or_path=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "use_fast_tokenizer=True\n",
    "finetuning_task=\"text-classification\",\n",
    "max_seq_length=512\n",
    "\n",
    "# training parameters\n",
    "pad_to_max_length = True\n",
    "max_train_samples=120000\n",
    "fp16 = False\n",
    "\n",
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name)\n",
    "label_list = raw_datasets['train'].unique(\"label\")\n",
    "# we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n",
    "label_list = [str(label) for label in label_list]\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    \n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "# return a dict\n",
    "    examples[\"sentence\"] = examples[text_column_name]\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"sentence\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[str(l)] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    # add additional keys: 'input_ids','token_type_ids', 'attention_mask','label'  \n",
    "    return result\n",
    "\n",
    "# test = preprocess_function(raw_datasets['train'][0])\n",
    "# Running the preprocessing pipeline on all the datasets\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Decoder(config = config)\n",
    "        self.pred_head = nn.Linear(self.encoder.config.d_model, config.num_labels, bias=False)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        last_hidden_state = self.encoder(batch['input_ids'])\n",
    "        batch_size = last_hidden_state.shape[0]\n",
    "        #print(last_hidden_state.shape)\n",
    "        if self.config.pad_token_id is not None:\n",
    "            sequence_length = (batch['input_ids'] == self.config.pad_token_id).int().argmax(-1) - 1\n",
    "            # when there is no pad_token, sequence_length = -1, we use module to make it to the last position\n",
    "            sequence_length = sequence_length % batch['input_ids'].shape[-1]\n",
    "            # if we use encoder_outputs.last_hidden_state[:,sequence_length], we got the selection on the second axis, which is wrong\n",
    "            hiddens = last_hidden_state[torch.arange(batch_size),sequence_length]\n",
    "        else:\n",
    "            # use the last token\n",
    "            hiddens = last_hidden_state[:,-1,:].squeeze()\n",
    "        \n",
    "        pred_out = self.pred_head(hiddens)\n",
    "        return pred_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dict = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"context_length\": 1024,\n",
    "    \"d_model\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"num_labels\": num_labels      \n",
    "}\n",
    "\n",
    "config = OmegaConf.create(config_dict)\n",
    "\n",
    "model = DecoderClassifier(config=config)\n",
    "\n",
    "\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    del batch['labels']\n",
    "    logits = model(batch)\n",
    "    print(logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2 tokenizer padding side is right\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertClassifier(model_name=model_name_or_path, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(model, inputs, labels, loss_fn):\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq):\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    step = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch = move_to_device(batch, device)\n",
    "        targets = batch['labels']\n",
    "        del batch['labels']\n",
    "        inputs = batch\n",
    "        \n",
    "        loss = compute_batch_loss(model, inputs, targets, loss_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        if step % output_freq == 0:\n",
    "            print(f\"steps: {step}, loss: {sum(training_loss)/step}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 50, loss: 1.4143715316057206\n",
      "steps: 100, loss: 1.4080633607506752\n",
      "steps: 150, loss: 1.402256528735161\n",
      "steps: 200, loss: 1.3959256412088872\n",
      "steps: 250, loss: 1.391684907078743\n",
      "steps: 300, loss: 1.3903517549236615\n",
      "steps: 350, loss: 1.388704954641206\n",
      "steps: 400, loss: 1.3889015892893075\n",
      "steps: 450, loss: 1.3831107773383458\n",
      "steps: 500, loss: 1.384534776031971\n",
      "steps: 550, loss: 1.38407462190498\n",
      "steps: 600, loss: 1.3841596693297227\n",
      "steps: 650, loss: 1.3843472303335482\n",
      "steps: 700, loss: 1.3839757493989808\n",
      "steps: 750, loss: 1.3836341782013575\n",
      "steps: 800, loss: 1.3834408656135202\n",
      "steps: 850, loss: 1.3837899052746157\n",
      "steps: 900, loss: 1.3836581347054906\n",
      "steps: 950, loss: 1.3837411808026465\n",
      "steps: 1000, loss: 1.3839117942154409\n",
      "steps: 1050, loss: 1.3838368797586078\n",
      "steps: 1100, loss: 1.383696310438893\n",
      "steps: 1150, loss: 1.3835738700628282\n",
      "steps: 1200, loss: 1.3835828377058108\n",
      "steps: 1250, loss: 1.3835020465135575\n",
      "steps: 1300, loss: 1.3834593165150055\n",
      "steps: 1350, loss: 1.3835363288499691\n",
      "steps: 1400, loss: 1.3835108833014964\n",
      "steps: 1450, loss: 1.383393730891162\n",
      "steps: 1500, loss: 1.3832646408279736\n",
      "steps: 1550, loss: 1.3832040577742362\n",
      "steps: 1600, loss: 1.3830501720495523\n",
      "steps: 1650, loss: 1.3832484043186362\n",
      "steps: 1700, loss: 1.383347071917618\n",
      "steps: 1750, loss: 1.3832904104675565\n",
      "steps: 1800, loss: 1.382947364466058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m model_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_dataloader, optimizer, loss_fn, model, device, output_freq)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m training_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m output_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 2e-3\n",
    "set_seed(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "model_trained = train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_dataloader, metrics, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = move_to_device(batch, device)\n",
    "            targets = batch['labels']\n",
    "            del batch['labels']\n",
    "            inputs = batch\n",
    "            model_output = model(inputs)\n",
    "            logits = model_output\n",
    "            \n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(targets.detach().cpu().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().tolist())\n",
    "            \n",
    "        result = metrics.compute(predictions=all_preds, references = all_labels)\n",
    "    print(result)\n",
    "    print(len(all_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9156578947368421}\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics(eval_dataloader, metric, model_trained, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
