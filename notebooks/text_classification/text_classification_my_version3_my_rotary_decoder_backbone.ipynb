{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from llm_lab.model.vanilla_decoder import VanillaDecoderModel\n",
    "from llm_lab.model.rotary_decoder import RotaryDecoderModel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "Decoder = RotaryDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data parameters\n",
    "dataset_name=\"fancyzhx/ag_news\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\"\n",
    "model_name_or_path=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "use_fast_tokenizer=True\n",
    "finetuning_task=\"text-classification\",\n",
    "max_seq_length=512\n",
    "\n",
    "batch_size = 512\n",
    "num_workers=16\n",
    "\n",
    "# training parameters\n",
    "pad_to_max_length = True\n",
    "max_train_samples=120000\n",
    "fp16 = False\n",
    "\n",
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name)\n",
    "label_list = raw_datasets['train'].unique(\"label\")\n",
    "# we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n",
    "label_list = [str(label) for label in label_list]\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    \n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "# return a dict\n",
    "    examples[\"sentence\"] = examples[text_column_name]\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"sentence\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[str(l)] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    # add additional keys: 'input_ids','token_type_ids', 'attention_mask','label'  \n",
    "    return result\n",
    "\n",
    "# test = preprocess_function(raw_datasets['train'][0])\n",
    "# Running the preprocessing pipeline on all the datasets\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=data_collator, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Decoder(config = config)\n",
    "        self.pred_head = nn.Linear(config.hidden_size, config.num_labels, bias=False)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        last_hidden_state = self.encoder(batch['input_ids'])\n",
    "        batch_size = last_hidden_state.shape[0]\n",
    "        #print(last_hidden_state.shape)\n",
    "        if self.config.pad_token_id is not None:\n",
    "            sequence_length = (batch['input_ids'] == self.config.pad_token_id).int().argmax(-1) - 1\n",
    "            # when there is no pad_token, sequence_length = -1, we use module to make it to the last position\n",
    "            sequence_length = sequence_length % batch['input_ids'].shape[-1]\n",
    "            # if we use encoder_outputs.last_hidden_state[:,sequence_length], we got the selection on the second axis, which is wrong\n",
    "            hiddens = last_hidden_state[torch.arange(batch_size),sequence_length]\n",
    "        else:\n",
    "            # use the last token\n",
    "            hiddens = last_hidden_state[:,-1,:].squeeze()\n",
    "        \n",
    "        pred_out = self.pred_head(hiddens)\n",
    "        return pred_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 4])\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dict = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"max_position_embeddings\": 1024,\n",
    "    \"hidden_size\": 128,\n",
    "    \"intermediate_size\": 512,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_key_value_heads\":1,\n",
    "    \"num_layers\": 2,\n",
    "    \"attention_dropout\":0.1,\n",
    "    \"rms_norm_eps\": 1e-6,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"o_bias\": True,\n",
    "    \"mlp_bias\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"num_labels\": num_labels,\n",
    "    \"rope_theta\": 10000,\n",
    "    \"causal_attention\": True\n",
    "}\n",
    "\n",
    "config = OmegaConf.create(config_dict)\n",
    "\n",
    "model = DecoderClassifier(config=config)\n",
    "\n",
    "\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    del batch['labels']\n",
    "    logits = model(batch)\n",
    "    print(logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2 tokenizer padding side is right\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertClassifier(model_name=model_name_or_path, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(model, inputs, labels, loss_fn):\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq):\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    step = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch = move_to_device(batch, device)\n",
    "        targets = batch['labels']\n",
    "        del batch['labels']\n",
    "        inputs = batch\n",
    "        \n",
    "        loss = compute_batch_loss(model, inputs, targets, loss_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        if step % output_freq == 0:\n",
    "            print(f\"steps: {step}, loss: {sum(training_loss)/step}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 50, loss: 1.4133142375946044\n",
      "steps: 100, loss: 1.1764977645874024\n",
      "steps: 150, loss: 0.9898621477683385\n",
      "steps: 200, loss: 0.8647661364078522\n",
      "steps: 50, loss: 0.3981755769252777\n",
      "steps: 100, loss: 0.3550341109931469\n",
      "steps: 150, loss: 0.3340720559159915\n",
      "steps: 200, loss: 0.3199392878264189\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-3\n",
    "set_seed(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "for _ in range(2):\n",
    "    train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_dataloader, metrics, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = move_to_device(batch, device)\n",
    "            targets = batch['labels']\n",
    "            del batch['labels']\n",
    "            inputs = batch\n",
    "            model_output = model(inputs)\n",
    "            logits = model_output\n",
    "            \n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(targets.detach().cpu().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().tolist())\n",
    "            \n",
    "        result = metrics.compute(predictions=all_preds, references = all_labels)\n",
    "    print(result)\n",
    "    print(len(all_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9006578947368421}\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics(eval_dataloader, metric, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
