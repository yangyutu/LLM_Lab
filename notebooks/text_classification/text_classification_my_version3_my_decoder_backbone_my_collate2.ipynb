{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from functools import partial\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from llm_lab.model.vanilla_decoder import VanillaDecoderModel\n",
    "from llm_lab.utils.collate_utils import default_data_collator_with_padding\n",
    "#from model.rotary_decoder import RotaryDecoder\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "Decoder = VanillaDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data parameters\n",
    "dataset_name=\"fancyzhx/ag_news\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\"\n",
    "model_name_or_path=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "use_fast_tokenizer=True\n",
    "finetuning_task=\"text-classification\",\n",
    "max_seq_length=512\n",
    "\n",
    "batch_size = 512\n",
    "num_workers=16\n",
    "\n",
    "# training parameters\n",
    "pad_to_max_length = True\n",
    "max_train_samples=120000\n",
    "fp16 = False\n",
    "\n",
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name)\n",
    "label_list = raw_datasets['train'].unique(\"label\")\n",
    "# we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n",
    "label_list = [str(label) for label in label_list]\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    \n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples, tokenizer_parameters):\n",
    "    # batch model; return a dict\n",
    "    examples[\"sentence\"] = examples[text_column_name]\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"sentence\"], **tokenizer_parameters)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        # batch processing a list of labels\n",
    "        result[\"label\"] = [label_to_id[str(l)] for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "# test = preprocess_function(raw_datasets['train'][0])\n",
    "# Running the preprocessing pipeline on all the datasets\n",
    "tokenizer_params = {\n",
    "    \"padding\": False,\n",
    "    \"max_length\": max_seq_length,\n",
    "    \"truncation\": True}\n",
    "\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    partial(preprocess_function, tokenizer_parameters=tokenizer_params),\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = partial(default_data_collator_with_padding, pad_token_id=tokenizer.pad_token_id, pad_to_multiple_of=8, padding_strategy=\"longest\")\n",
    "# if pad_to_max_length:\n",
    "#     data_collator = default_data_collator\n",
    "# elif fp16:\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "# else:\n",
    "#     data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'label': tensor([2]),\n",
       "             'input_ids': tensor([[22401,   520,    13, 15682, 30358,  5157, 20008,   262,  2619,   357,\n",
       "                      12637,     8,  8428,   532, 10073,    12,  7255,   364,    11,  5007,\n",
       "                       3530,   338, 45215,    59,  3903,   286, 14764,    12,   948,    77,\n",
       "                        873,    11,   389,  4379,  4077,   757,    13, 50256, 50256, 50256]]),\n",
       "             'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=data_collator, num_workers=num_workers)\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attention_mask', 'input_ids']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list({'attention_mask', 'input_ids'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Decoder(config = config)\n",
    "        self.pred_head = nn.Linear(self.encoder.config.d_model, config.num_labels, bias=False)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        last_hidden_state = self.encoder(batch['input_ids'])\n",
    "        batch_size = last_hidden_state.shape[0]\n",
    "        #print(last_hidden_state.shape)\n",
    "        if self.config.pad_token_id is not None:\n",
    "            sequence_length = (batch['input_ids'] == self.config.pad_token_id).int().argmax(-1) - 1\n",
    "            # when there is no pad_token, sequence_length = -1, we use module to make it to the last position\n",
    "            sequence_length = sequence_length % batch['input_ids'].shape[-1]\n",
    "            # if we use encoder_outputs.last_hidden_state[:,sequence_length], we got the selection on the second axis, which is wrong\n",
    "            hiddens = last_hidden_state[torch.arange(batch_size),sequence_length]\n",
    "        else:\n",
    "            # use the last token\n",
    "            hiddens = last_hidden_state[:,-1,:].squeeze()\n",
    "        \n",
    "        pred_out = self.pred_head(hiddens)\n",
    "        return pred_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla decoder model 3\n",
      "torch.Size([512, 4])\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dict = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"context_length\": 1024,\n",
    "    \"d_model\": 128,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"num_labels\": num_labels,\n",
    "    \"causal_attention\": True\n",
    "}\n",
    "\n",
    "config = OmegaConf.create(config_dict)\n",
    "\n",
    "model = DecoderClassifier(config=config)\n",
    "\n",
    "\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    del batch['label']\n",
    "    logits = model(batch)\n",
    "    print(logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2 tokenizer padding side is right\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertClassifier(model_name=model_name_or_path, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            batch[k] = v.to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(model, inputs, labels, loss_fn):\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq):\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    step = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch = move_to_device(batch, device)\n",
    "        targets = batch['label']\n",
    "        del batch['label']\n",
    "        inputs = batch\n",
    "        \n",
    "        loss = compute_batch_loss(model, inputs, targets, loss_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        if step % output_freq == 0:\n",
    "            print(f\"steps: {step}, loss: {sum(training_loss)/step}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 50, loss: 1.5106132340431213\n",
      "steps: 100, loss: 1.3352366852760316\n",
      "steps: 150, loss: 1.1725797645250957\n",
      "steps: 200, loss: 1.0333916260302067\n",
      "steps: 50, loss: 0.4339019471406937\n",
      "steps: 100, loss: 0.37769288033246995\n",
      "steps: 150, loss: 0.3457512793938319\n",
      "steps: 200, loss: 0.3213794261217117\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-3\n",
    "set_seed(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "for _ in range(2):\n",
    "    train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_dataloader, metrics, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = move_to_device(batch, device)\n",
    "            targets = batch['label']\n",
    "            del batch['label']\n",
    "            inputs = batch\n",
    "            model_output = model(inputs)\n",
    "            logits = model_output\n",
    "            \n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(targets.detach().cpu().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().tolist())\n",
    "            \n",
    "        result = metrics.compute(predictions=all_preds, references = all_labels)\n",
    "    print(result)\n",
    "    print(len(all_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.905921052631579}\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics(eval_dataloader, metric, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
