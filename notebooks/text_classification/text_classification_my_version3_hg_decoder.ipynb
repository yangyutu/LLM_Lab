{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyutu/miniconda3/envs/huggingface_lastest/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from vanila_decoder import Decoder\n",
    "from omegaconf import OmegaConf\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data parameters\n",
    "dataset_name=\"fancyzhx/ag_news\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\"\n",
    "model_name_or_path=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "use_fast_tokenizer=True\n",
    "finetuning_task=\"text-classification\",\n",
    "max_seq_length=512\n",
    "\n",
    "# training parameters\n",
    "pad_to_max_length = True\n",
    "max_train_samples=120000\n",
    "fp16 = False\n",
    "\n",
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name)\n",
    "label_list = raw_datasets['train'].unique(\"label\")\n",
    "# we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n",
    "label_list = [str(label) for label in label_list]\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    \n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "# return a dict\n",
    "    examples[\"sentence\"] = examples[text_column_name]\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"sentence\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[str(l)] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    # add additional keys: 'input_ids','token_type_ids', 'attention_mask','label'  \n",
    "    return result\n",
    "\n",
    "# test = preprocess_function(raw_datasets['train'][0])\n",
    "# Running the preprocessing pipeline on all the datasets\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at stanford-crfm/battlestar-gpt2-small-x49 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path,\n",
    "                                    num_labels=num_labels,\n",
    "                                    finetuning_task=finetuning_task)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderClassifier(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Decoder(config)\n",
    "#         self.pred_head = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "        \n",
    "#     def forward(self, batch):\n",
    "        \n",
    "#         encoder_outputs = self.encoder(**batch)\n",
    "#         #print(encoder_outputs.last_hidden_state.shape)\n",
    "#         pred_out = self.pred_head(encoder_outputs.last_hidden_state[:,0,:].squeeze())\n",
    "#         return pred_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf\n",
    "\n",
    "# config_dict = {\n",
    "#     \"vocab_size\": tokenizer.vocab_size,\n",
    "#     \"context_length\": 1024,\n",
    "#     \"d_model\": 768,\n",
    "#     \"num_heads\": 12,\n",
    "#     \"num_layers\": 12,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"qkv_bias\": False,      \n",
    "# }\n",
    "\n",
    "# config = OmegaConf.create(config_dict)\n",
    "\n",
    "# model = DecoderClassifier(config, num_labels=num_labels)\n",
    "\n",
    "# text = \"Hello\"\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "# print(model(input_ids))\n",
    "    \n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertClassifier(model_name=model_name_or_path, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(model, inputs, labels, loss_fn):\n",
    "    \n",
    "    logits = model(**inputs).logits\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq):\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    step = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch = move_to_device(batch, device)\n",
    "        targets = batch['labels']\n",
    "        del batch['labels']\n",
    "        inputs = batch\n",
    "        \n",
    "        loss = compute_batch_loss(model, inputs, targets, loss_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        if step % output_freq == 0:\n",
    "            print(f\"steps: {step}, loss: {sum(training_loss)/step}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 500, loss: 0.5484127519796602\n",
      "steps: 1000, loss: 0.4141420872246381\n",
      "steps: 1500, loss: 0.3646747649149038\n",
      "steps: 2000, loss: 0.33875616538419856\n",
      "steps: 2500, loss: 0.31467409339798613\n",
      "steps: 3000, loss: 0.3001080111132469\n",
      "steps: 3500, loss: 0.2903755627794058\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-5\n",
    "set_seed(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "model_trained = train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_dataloader, metrics, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = move_to_device(batch, device)\n",
    "            targets = batch['labels']\n",
    "            del batch['labels']\n",
    "            inputs = batch\n",
    "            model_output = model(**inputs)\n",
    "            logits = model_output.logits\n",
    "            \n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(targets.detach().cpu().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().tolist())\n",
    "            \n",
    "        result = metrics.compute(predictions=all_preds, references = all_labels)\n",
    "    print(result)\n",
    "    print(len(all_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9132894736842105}\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics(eval_dataloader, metric, model_trained, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
