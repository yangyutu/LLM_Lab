{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data parameters\n",
    "dataset_name=\"fancyzhx/ag_news\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\"\n",
    "model_name_or_path=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "model_name_or_path=\"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "use_fast_tokenizer=True\n",
    "finetuning_task=\"text-classification\",\n",
    "max_seq_length=512\n",
    "\n",
    "# training parameters\n",
    "pad_to_max_length = True\n",
    "max_train_samples=120000\n",
    "fp16 = False\n",
    "\n",
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=64): 100%|██████████| 120000/120000 [00:04<00:00, 28498.09 examples/s]\n",
      "Running tokenizer on dataset (num_proc=64): 100%|██████████| 7600/7600 [00:00<00:00, 9381.80 examples/s] \n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(dataset_name)\n",
    "label_list = raw_datasets['train'].unique(\"label\")\n",
    "# we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n",
    "label_list = [str(label) for label in label_list]\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    \n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "# return a dict\n",
    "    examples[\"sentence\"] = examples[text_column_name]\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"sentence\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[str(l)] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    # add additional keys: 'input_ids','token_type_ids', 'attention_mask','label'  \n",
    "    return result\n",
    "\n",
    "# test = preprocess_function(raw_datasets['train'][0])\n",
    "# Running the preprocessing pipeline on all the datasets\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DecoderClassifier(nn.Module):\n",
    "    def __init__(self, model_name_or_path, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        model_config.n_layer = config.num_layers\n",
    "        encoder = AutoModel.from_config(model_config)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.pred_head = nn.Linear(self.encoder.config.hidden_size, config.num_labels, bias=False)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        encoder_outputs = self.encoder(**batch)\n",
    "        batch_size = encoder_outputs.last_hidden_state.shape[0]\n",
    "        #print(encoder_outputs.last_hidden_state.shape)\n",
    "        if self.config.pad_token_id is not None:\n",
    "            sequence_length = (batch['input_ids'] == self.config.pad_token_id).int().argmax(-1) - 1\n",
    "            # when there is no pad_token, sequence_length = -1, we use module to make it to the last position\n",
    "            sequence_length = sequence_length % batch['input_ids'].shape[-1]\n",
    "            # if we use encoder_outputs.last_hidden_state[:,sequence_length], we got the selection on the second axis, which is wrong\n",
    "            hiddens = encoder_outputs.last_hidden_state[torch.arange(batch_size),sequence_length]\n",
    "        else:\n",
    "            # use the last token\n",
    "            hiddens = encoder_outputs.last_hidden_state[:,-1,:].squeeze()\n",
    "        \n",
    "        pred_out = self.pred_head(hiddens)\n",
    "        return pred_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dict = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"context_length\": 1024,\n",
    "    \"d_model\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"num_labels\": num_labels      \n",
    "}\n",
    "\n",
    "config = OmegaConf.create(config_dict)\n",
    "\n",
    "model = DecoderClassifier(model_name_or_path=model_name_or_path, config=config)\n",
    "\n",
    "\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    del batch['labels']\n",
    "    logits = model(batch)\n",
    "    print(logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2 tokenizer padding side is right\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertClassifier(model_name=model_name_or_path, config=config, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(model, inputs, labels, loss_fn):\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq):\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    step = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch = move_to_device(batch, device)\n",
    "        targets = batch['labels']\n",
    "        del batch['labels']\n",
    "        inputs = batch\n",
    "        \n",
    "        loss = compute_batch_loss(model, inputs, targets, loss_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        if step % output_freq == 0:\n",
    "            print(f\"steps: {step}, loss: {sum(training_loss)/step}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "set_seed(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "model_trained = train_one_epoch(train_dataloader, optimizer, loss_fn, model, device, output_freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_dataloader, metrics, model, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = move_to_device(batch, device)\n",
    "            targets = batch['labels']\n",
    "            del batch['labels']\n",
    "            inputs = batch\n",
    "            model_output = model(inputs)\n",
    "            logits = model_output\n",
    "            \n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(targets.detach().cpu().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().tolist())\n",
    "            \n",
    "        result = metrics.compute(predictions=all_preds, references = all_labels)\n",
    "    print(result)\n",
    "    print(len(all_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9046052631578947}\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics(eval_dataloader, metric, model_trained, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
