{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyutu/miniconda3/envs/huggingface_lastest/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_lab.model.rotary_decoder import RotaryDecoderModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = RotaryDecoderModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, input_ids, use_cache=False, start_pos=0):\n",
    "        hidden_states = self.decoder(input_ids=input_ids, use_cache=use_cache, start_pos=start_pos)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "model_name=\"stanford-crfm/battlestar-gpt2-small-x49\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dict = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"max_position_embeddings\": 1024,\n",
    "    \"hidden_size\": 768,         # model dimension\n",
    "    \"intermediate_size\": 768*4,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"num_heads\": 4,          # Number of attention heads\n",
    "    \"num_layers\": 6,         # Number of layers\n",
    "    \"attention_dropout\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False,       # Query-key-value bias\n",
    "    \"o_bias\": True,\n",
    "    \"mlp_bias\": True,\n",
    "    \"rms_norm_eps\": 1e-6,\n",
    "    \"dropout\": 0.1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"causal_attention\": True,\n",
    "    \"use_cache\": True,\n",
    "    \"cache_max_batch_size\":128,\n",
    "    \"cache_max_seq_len\": 128\n",
    "}\n",
    "\n",
    "config = OmegaConf.create(config_dict)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_935/3475930062.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RotaryCausalLM(config)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model_state_dict = torch.load(\"model.pth\")\n",
    "\n",
    "model.load_state_dict(model_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, \n",
    "             tokenizer,\n",
    "             prompts: List[str],\n",
    "             device, \n",
    "             greedy_decoding: bool=False,\n",
    "             temperature: float=0.8,\n",
    "             top_p: float=0.9,\n",
    "             max_new_tokens: int=100):\n",
    "    \n",
    "    \n",
    "    prompt_tokens = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    \n",
    "    max_prompt_len = max([len(prompt_token) for prompt_token in prompt_tokens])\n",
    "    \n",
    "    batch_size = len(prompt_tokens)\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    total_len = max_prompt_len + max_new_tokens\n",
    "    \n",
    "    # every token is defaulted to pad_token_id\n",
    "    tokens = torch.full((batch_size, total_len), pad_token_id, dtype=torch.long, device=device)\n",
    "    prompt_pad_mask = tokens == pad_token_id # True if the token is a prompt token, False otherwise\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        # fill in existing prompt tokens\n",
    "        tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "    eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "    \n",
    "    for cur_pos in range(total_len - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens[:,cur_pos:cur_pos + 1], use_cache=True, start_pos=cur_pos)\n",
    "\n",
    "            if greedy_decoding:\n",
    "                next_token = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        \n",
    "        # only replace toekn if it is a padding token\n",
    "        next_token = torch.where(prompt_pad_mask[:, cur_pos+1], next_token, tokens[:,cur_pos + 1])\n",
    "        \n",
    "        tokens[:, cur_pos + 1] = next_token\n",
    "        \n",
    "        # EOS is reachehed only if we found an EOS token for a padding position\n",
    "        \n",
    "        eos_reached |= (prompt_pad_mask[:, cur_pos + 1]) & (next_token == eos_token_id)\n",
    "        \n",
    "        if all(eos_reached):\n",
    "            break\n",
    "        \n",
    "    out_tokens = []\n",
    "    out_text = []\n",
    "    \n",
    "    for current_prompt_tokens in tokens.tolist():\n",
    "        # cut to the EOS token if present\n",
    "        if eos_token_id in current_prompt_tokens:\n",
    "            eos_idx = current_prompt_tokens.index(eos_token_id)\n",
    "            current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "        \n",
    "        out_tokens.append(current_prompt_tokens)\n",
    "        out_text.append(tokenizer.decode(current_prompt_tokens))\n",
    "    \n",
    "    return (out_tokens, out_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tokens, out_text = generate(model, tokenizer, prompts=prompts, device=device, greedy_decoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello guitar guitar guitar guitar guitar guitar guitar guitarvillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevillevilleville provided provided guitar guitar headquarters headquarters headquarters headquartersont guitar guitarvillevillevillevillevillevillevillevillevillevillevillevillevillevilleville']\n"
     ]
    }
   ],
   "source": [
    "print(out_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
